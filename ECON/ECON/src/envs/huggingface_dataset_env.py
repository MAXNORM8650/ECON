import gym
from gym import spaces
import numpy as np
from datasets import load_dataset, Dataset, IterableDataset
from typing import Dict, Any, Optional, Tuple, List
from loguru import logger
import re
import torch
import torch.nn.functional as F
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random

class HuggingFaceDatasetEnv(gym.Env):
    metadata = {'render_modes': ['human'], 'render_fps': 4}

    def __init__(self, **kwargs):
        super().__init__()
        
        self.dataset_path = kwargs.get("hf_dataset_path", "gsm8k")
        self.dataset_config_name = kwargs.get("hf_dataset_config_name", None)
        self.dataset_split = kwargs.get("dataset_split", "train")
        self.is_streaming = kwargs.get("dataset_streaming", False)
        self.use_random_sampling = kwargs.get("use_random_sampling", True)  # 添加随机采样选项
        self.use_dataset_episode = kwargs.get("use_dataset_episode", False)  # 数据集级别episode选项
        
        self.question_field = kwargs.get("question_field_name", "question")
        self.answer_field = kwargs.get("answer_field_name", "answer")
        
        # For reward calculation, if needed directly in env
        self.reward_args = kwargs.get("reward_config", {})

        try:
            self.dataset = load_dataset(
                self.dataset_path, 
                name=self.dataset_config_name, 
                split=self.dataset_split,
                streaming=self.is_streaming
            )
            if self.is_streaming:
                self.dataset_iterator = iter(self.dataset)
                # For IterableDataset, we can't easily get the length.
                # We might need a max_episodes arg from config for termination in streaming mode.
                logger.info(f"Loaded IterableDataset: {self.dataset_path}, split: {self.dataset_split}")
            else:
                self.dataset_list = list(self.dataset) # Convert to list for easier iteration and shuffling if needed
                self.dataset_iterator = None # Will be created in reset
                self.current_data_idx = -1
                self.num_samples = len(self.dataset_list)
                logger.info(f"Loaded Dataset: {self.dataset_path}, split: {self.dataset_split}, num_samples: {self.num_samples}")
                
                # 如果使用随机采样，打乱数据集
                if self.use_random_sampling and not self.use_dataset_episode:
                    random.shuffle(self.dataset_list)
                    logger.info("Dataset shuffled for random sampling")

        except Exception as e:
            logger.error(f"Failed to load dataset '{self.dataset_path}' (config: {self.dataset_config_name}, split: {self.dataset_split}): {e}")
            raise

        self.max_question_length = kwargs.get("max_question_length", 1024)
        self.max_answer_length = kwargs.get("max_answer_length", 1024) # For action space

        # Define action space - what the agent "outputs" to the environment
        # For LLMs, this is typically the generated text.
        # Using gym.spaces.Text requires gym version 0.26+
        # As a placeholder, or if direct text passing is used, can be simplified.
        self.action_space = spaces.Text(max_length=self.max_answer_length)

        # Define observation space - what the agent "sees"
        # This will be the question text.
        self.observation_space = spaces.Text(max_length=self.max_question_length)
        
        # Current sample from the dataset
        self.current_sample: Optional[Dict] = None
        self.current_question: Optional[str] = None
        self.current_ground_truth_answer: Optional[str] = None
        self.episode_count = 0  # 添加episode计数器，用于追踪问题
        
        # 数据集级别episode的状态追踪
        if self.use_dataset_episode:
            self.step_count = 0  # 当前episode内的步数
            self.episode_limit = self.num_samples if not self.is_streaming else 1000  # 使用数据集大小作为episode限制
            self.current_episode_samples = []  # 当前episode处理的所有样本
            self.episode_results = []  # 当前episode的所有结果
        else:
            # Episode specifics (each question is an episode)
            self.episode_length = 0 # Steps within current episode (always 1)
            self.episode_limit = 1

    def _get_next_sample(self) -> Optional[Dict]:
        if self.is_streaming:
            try:
                return next(self.dataset_iterator)
            except StopIteration:
                logger.info("Streaming dataset iterator exhausted.")
                return None
        else:
            if self.use_dataset_episode:
                # 数据集级别episode：顺序遍历所有样本
                self.current_data_idx += 1
                if self.current_data_idx < self.num_samples:
                    return self.dataset_list[self.current_data_idx]
                else:
                    logger.info("Dataset-level episode completed: all samples processed.")
                    return None
            elif self.use_random_sampling:
                # 随机采样：每次随机选择一个样本
                if self.num_samples > 0:
                    random_idx = random.randint(0, self.num_samples - 1)
                    sample = self.dataset_list[random_idx]
                    logger.debug(f"Random sampling: selected index {random_idx}")
                    return sample
                else:
                    logger.info("Dataset is empty.")
                    return None
            else:
                # 顺序采样：原有逻辑
                self.current_data_idx += 1
                if self.current_data_idx < self.num_samples:
                    return self.dataset_list[self.current_data_idx]
                else:
                    logger.info("Non-streaming dataset iterator exhausted.")
                    return None

    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[Any, Dict[str, Any]]:
        super().reset(seed=seed) # Gym 0.26+
        
        # 设置随机种子以确保可重现性
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        if self.use_dataset_episode:
            # 数据集级别episode：重置状态，从第一个样本开始
            self.current_data_idx = -1  # 会被_get_next_sample递增到0
            self.step_count = 0
            self.current_episode_samples = []
            self.episode_results = []
            self.episode_count += 1
            logger.info(f"Starting dataset-level Episode {self.episode_count}: will process {self.num_samples} samples")
        else:
            # 原有逻辑：每个问题一个episode
            if not self.is_streaming and (self.dataset_iterator is None or self.current_data_idx >= self.num_samples -1):
                # Re-initialize iterator for non-streaming dataset if exhausted or first time
                if not self.use_random_sampling:
                    # 只有在顺序采样时才重置索引
                    self.current_data_idx = -1 # Will be incremented by _get_next_sample

        self.current_sample = self._get_next_sample()
        
        if self.current_sample is None:
            # Handle dataset exhaustion, e.g., by raising an error or returning a special state
            # For now, let's raise an error to make it explicit during development.
            # In a long run, might want to loop the dataset or have a max_episodes from config.
            raise StopIteration("Dataset exhausted. Implement looping or max_episode limit if needed.")

        self.current_question = str(self.current_sample.get(self.question_field, ""))
        self.current_ground_truth_answer = str(self.current_sample.get(self.answer_field, ""))
        
        if not self.use_dataset_episode:
            self.episode_length = 0
            self.episode_count += 1
        
        # 记录问题变化以便调试
        if self.use_dataset_episode:
            question_preview = self.current_question[:100] + "..." if len(self.current_question) > 100 else self.current_question
            logger.info(f"Episode {self.episode_count}, Step {self.step_count + 1}/{self.num_samples}: {question_preview}")
        else:
            question_preview = self.current_question[:100] + "..." if len(self.current_question) > 100 else self.current_question
            logger.info(f"Episode {self.episode_count}: New question - {question_preview}")
        
        # Observation is the question text
        # Preprocess if necessary (e.g., tokenization if obs_space was Box)
        # For now, passing raw text, MAC needs to handle it.
        observation = self.current_question 
        
        info = {"sample": self.current_sample} # Pass the whole sample for potential use in reward or logging
        return observation, info

    def step(self, action: Any, extra_info: Optional[Dict[str, Any]] = None) -> Tuple[Any, float, bool, bool, Dict[str, Any]]:
        """
        Execute one step in the environment.
        
        Args:
            action: The primary action (final answer string from coordinator/agent)
            extra_info: Additional information for reward calculation including:
                - 'agent_responses': List[str] - Individual agent responses
                - 'commitment_text': str - Coordinator's commitment text
                - 'agent_log_probs': Optional[List[float]] - Token log probabilities for AL reward
                - 'prompt_embeddings': Optional[torch.Tensor] - Agent prompt embeddings
                - 'belief_states': Optional[torch.Tensor] - Agent belief states
        """
        if self.current_sample is None:
            raise RuntimeError("step() called before reset() or after dataset exhaustion.")

        if self.use_dataset_episode:
            self.step_count += 1
        else:
            self.episode_length += 1
        
        # Extract primary action and additional info
        if isinstance(action, dict):
            llm_answer_str = str(action.get("answer", ""))
            if extra_info is None:
                extra_info = action  # Use action dict as extra_info if not provided separately
        else:
            llm_answer_str = str(action)
            
        if extra_info is None:
            extra_info = {}

        # === 清晰显示完整推理过程 ===
        logger.info("=" * 80)
        logger.info(f"🔍 QUESTION: {self.current_question}")
        logger.info("=" * 80)
        
        # Strategy will be logged by MAC
        
        # Executor responses will be logged by MAC
        
        # Coordinator commitment will be logged by MAC
        
        logger.info(f"📖 GROUND TRUTH: {self.current_ground_truth_answer}")
        logger.info("=" * 80)

        # --- Reward Calculation ---
        # Task-Specific (TS) reward: 基于正确性
        is_correct = self._evaluate_answer(llm_answer_str, self.current_ground_truth_answer)
        reward_ts = 1.0 if is_correct else 0.0
        
        # Action Likelihood (AL) reward: 基于动作似然性
        reward_al = self._calculate_action_likelihood_reward(extra_info)
        
        # Collaborative Contribution (CC) reward: 基于协作贡献
        reward_cc = self._calculate_collaborative_contribution_reward(
            llm_answer_str, extra_info, is_correct
        )
        
        # 根据配置权重计算总奖励
        al_weight = getattr(self.reward_args, 'al_weight', 0.3)
        ts_weight = getattr(self.reward_args, 'ts_weight', 0.5)
        cc_weight = getattr(self.reward_args, 'cc_weight', 0.2)
        
        total_reward = al_weight * reward_al + ts_weight * reward_ts + cc_weight * reward_cc
        
        # 显示奖励信息
        logger.info(f"🎯 REWARD BREAKDOWN:")
        logger.info(f"   TS (Task-Specific): {reward_ts:.3f} * {ts_weight:.1f} = {reward_ts * ts_weight:.3f}")
        logger.info(f"   AL (Action Likelihood): {reward_al:.3f} * {al_weight:.1f} = {reward_al * al_weight:.3f}")
        logger.info(f"   CC (Collaborative): {reward_cc:.3f} * {cc_weight:.1f} = {reward_cc * cc_weight:.3f}")
        logger.info(f"   TOTAL REWARD: {total_reward:.3f}")
        logger.info("=" * 80)
        
        # --- 数据集级别episode的特殊处理 ---
        if self.use_dataset_episode:
            # 记录当前样本结果
            step_result = {
                "question": self.current_question,
                "ground_truth": self.current_ground_truth_answer,
                "llm_answer": llm_answer_str,
                "is_correct": is_correct,
                "reward_ts": reward_ts,
                "reward_al": reward_al,
                "reward_cc": reward_cc,
                "total_reward": total_reward
            }
            self.current_episode_samples.append(self.current_sample)
            self.episode_results.append(step_result)
            
            # 检查是否完成整个数据集
            terminated = (self.step_count >= self.num_samples)
            
            if not terminated:
                # 获取下一个样本
                self.current_sample = self._get_next_sample()
                if self.current_sample is None:
                    terminated = True
                else:
                    self.current_question = str(self.current_sample.get(self.question_field, ""))
                    self.current_ground_truth_answer = str(self.current_sample.get(self.answer_field, ""))
            
            if terminated:
                # Episode结束，计算整体统计
                total_correct = sum(1 for r in self.episode_results if r["is_correct"])
                accuracy = total_correct / len(self.episode_results) if self.episode_results else 0.0
                avg_reward = sum(r["total_reward"] for r in self.episode_results) / len(self.episode_results) if self.episode_results else 0.0
                
                logger.info(f"📊 DATASET-LEVEL EPISODE {self.episode_count} COMPLETED:")
                logger.info(f"   Total samples: {len(self.episode_results)}")
                logger.info(f"   Correct answers: {total_correct}")
                logger.info(f"   Accuracy: {accuracy:.3f}")
                logger.info(f"   Average reward: {avg_reward:.3f}")
                logger.info("=" * 80)
                
                next_observation = ""  # Episode结束，无下一个观察
            else:
                next_observation = self.current_question  # 下一个问题作为下一个观察
        else:
            # 原有逻辑：每个问题一个episode
            terminated = True
            next_observation = ""  # Placeholder

        truncated = False # Not typically used if episode length is fixed at 1 or based on dataset size
        
        info = {
            "is_correct": is_correct,
            "reward_ts": reward_ts,
            "reward_al": reward_al,
            "reward_cc": reward_cc,
            "llm_answer": llm_answer_str,
            "ground_truth_answer": self.current_ground_truth_answer
        }
        
        # 为数据集级别episode添加额外信息
        if self.use_dataset_episode:
            info.update({
                "step_count": self.step_count,
                "total_steps": self.num_samples,
                "progress": self.step_count / self.num_samples
            })
            
            if terminated:
                # 添加整体统计信息
                total_correct = sum(1 for r in self.episode_results if r["is_correct"])
                info.update({
                    "episode_accuracy": total_correct / len(self.episode_results) if self.episode_results else 0.0,
                    "episode_avg_reward": sum(r["total_reward"] for r in self.episode_results) / len(self.episode_results) if self.episode_results else 0.0,
                    "total_samples_processed": len(self.episode_results)
                })
        
        return next_observation, total_reward, terminated, truncated, info

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """Extracts content from \\boxed{} with improved fallback mechanisms."""
        if not isinstance(text, str): # Ensure text is a string
            return None
        
        # Primary: Look for \\boxed{content}
        match = re.search(r"\\boxed\{([\s\S]*?)\}", text)
        if match:
            content = match.group(1).strip()
            return content if content else None
        
        # Fallback 1: Look for boxed{content} without backslash
        match = re.search(r"boxed\{([\s\S]*?)\}", text)
        if match:
            content = match.group(1).strip()
            logger.info(f"Found 'boxed{{}}' without backslash: {content}")
            return content if content else None
        
        # Fallback 2: Look for the last number in the text (often the final answer)
        numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", text)
        last_number_candidate = numbers[-1] if numbers else None
        
        # Fallback 3: Look for "answer is" patterns, but prefer last number if found
        patterns = [
            r"(?:answer is|answer:|final answer is|final answer:|the answer is)\s*([+-]?\d+(?:\.\d+)?)",
            r"(?:therefore|thus|so)\s*[^0-9]*([+-]?\d+(?:\.\d+)?)",
            r"(?:equals|=)\s*([+-]?\d+(?:\.\d+)?)",
        ]
        
        pattern_matches = []
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                pattern_matches.append(match.group(1))
        
        # Strategy: If we have both pattern matches and a last number, 
        # prefer the last number if it appears in the later part of the text
        if last_number_candidate and pattern_matches:
            # Check if the last number appears after the pattern matches
            last_number_pos = text.rfind(last_number_candidate)
            pattern_positions = []
            for pattern_match in pattern_matches:
                pos = text.rfind(pattern_match)
                if pos != -1:
                    pattern_positions.append(pos)
            
            # If last number appears after pattern matches, prefer it
            if pattern_positions and last_number_pos > max(pattern_positions):
                logger.info(f"Using last number as it appears after pattern matches: {last_number_candidate}")
                return last_number_candidate
            elif pattern_matches:
                logger.info(f"Using pattern match: {pattern_matches[0]}")
                return pattern_matches[0]
        
        # If only pattern matches exist
        if pattern_matches:
            logger.info(f"Using pattern match: {pattern_matches[0]}")
            return pattern_matches[0]
        
        # If only last number exists
        if last_number_candidate:
            logger.info(f"Using last number in text as fallback: {last_number_candidate}")
            return last_number_candidate
        
        logger.warning(f"No answer found in text: {text[:100]}...")
        return None

    def _normalize_number_string(self, s: Optional[str]) -> Optional[str]:
        """Normalizes a string potentially representing a number."""
        if s is None:
            return None
        # Remove commas used as thousand separators
        s_no_commas = s.replace(",", "")
        # Remove trailing ".0" or ".00" etc. to treat 123.0 as 123 for int comparison
        # but keep 123.5 as 123.5
        if '.' in s_no_commas:
            parts = s_no_commas.split('.')
            if len(parts) == 2 and all(c == '0' for c in parts[1]):
                return parts[0] # Return only integer part if fractional part is all zeros
        return s_no_commas

    def _evaluate_answer(self, llm_answer: str, ground_truth_answer: str) -> bool:
        logger.debug(f"Evaluating LLM Answer: '{llm_answer}' vs Ground Truth: '{ground_truth_answer}'")

        llm_boxed_content = self._extract_boxed_content(llm_answer)
        gt_boxed_content = self._extract_boxed_content(ground_truth_answer)

        logger.debug(f"Boxed Content - LLM: '{llm_boxed_content}', GT: '{gt_boxed_content}'")

        # 如果两个都没有boxed内容，尝试直接从文本中提取数字
        if llm_boxed_content is None and gt_boxed_content is None:
            logger.info("Both answers lack \\boxed{} format, attempting direct text comparison")
            # 尝试从文本中提取最后的数字
            llm_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", llm_answer)
            gt_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", ground_truth_answer)
            
            if llm_numbers and gt_numbers:
                llm_boxed_content = llm_numbers[-1]
                gt_boxed_content = gt_numbers[-1]
                logger.info(f"Extracted numbers - LLM: '{llm_boxed_content}', GT: '{gt_boxed_content}'")
            else:
                logger.info(f"Evaluation failed: Unable to extract numerical answers. LLM: '{llm_answer[:100]}...', GT: '{ground_truth_answer[:100]}...'")
                return False

        # 如果只有一个有boxed内容，这通常表示格式问题
        if llm_boxed_content is None or gt_boxed_content is None:
            logger.info(f"Evaluation failed: Inconsistent answer formats. LLM boxed: '{llm_boxed_content}', GT boxed: '{gt_boxed_content}'")
            logger.info(f"Full answers - LLM: '{llm_answer[:150]}...', GT: '{ground_truth_answer[:150]}...'")
            return False

        # Normalize the string content from \boxed{} before attempting float conversion or string comparison
        norm_llm_content = self._normalize_number_string(llm_boxed_content)
        norm_gt_content = self._normalize_number_string(gt_boxed_content)
        
        logger.debug(f"Normalized Boxed Content - LLM: '{norm_llm_content}', GT: '{norm_gt_content}'")

        if norm_llm_content is None or norm_gt_content is None: # Should not happen if _extract_boxed_content returned non-None
             return False

        try:
            # Attempt to convert both to floats for numerical comparison
            llm_val = float(norm_llm_content)
            gt_val = float(norm_gt_content)

            # Check for near-equality
            if abs(llm_val - gt_val) < 1e-5:
                logger.info(f"✅ Correct answer: {llm_val} == {gt_val}")
                return True
            else:
                logger.info(f"❌ Numeric mismatch: LLM val {llm_val} vs GT val {gt_val}")
                return False
        except ValueError:
            # If conversion to float fails, fall back to string comparison
            logger.debug(f"ValueError converting to float. Comparing normalized strings: '{norm_llm_content}' vs '{norm_gt_content}'")
            if norm_llm_content == norm_gt_content:
                logger.info(f"✅ Correct answer (string match): '{norm_llm_content}'")
                return True
            else:
                # Last resort: compare the original (just stripped) boxed content
                if llm_boxed_content.strip() == gt_boxed_content.strip():
                    logger.info(f"✅ Correct answer (original content match): '{llm_boxed_content.strip()}'")
                    return True
                logger.info(f"❌ String mismatch after float conversion failed. LLM: '{norm_llm_content}', GT: '{norm_gt_content}'")
                return False

    def _calculate_action_likelihood_reward(self, extra_info: Dict[str, Any]) -> float:
        """
        计算动作似然性奖励 r^AL
        基于以下因素：
        1. Agent响应的一致性和质量
        2. 响应与commitment的相似度
        3. 响应的正确性（通过数字匹配检查）
        4. 如果API失败或响应无效，返回0.0
        """
        try:
            # 检查是否有有效的agent响应
            agent_responses = extra_info.get('agent_responses', [])
            commitment_text = extra_info.get('commitment_text', '')
            
            # 如果没有有效响应或响应包含错误信息，返回0.0
            if not agent_responses or not commitment_text:
                return 0.0
            
            # 检查响应是否包含API错误信息
            error_indicators = ['Error: Could not generate response', 'API Error', 'HTTP error', 'Failed to generate']
            for response in agent_responses:
                if any(error in str(response) for error in error_indicators):
                    return 0.0
            
            if any(error in str(commitment_text) for error in error_indicators):
                return 0.0
            
            # 过滤掉包含错误的响应
            valid_responses = [resp for resp in agent_responses 
                             if not any(error in str(resp) for error in error_indicators)]
            
            if not valid_responses:
                return 0.0
            
            # 方法1: 检查响应中数字答案的一致性
            response_numbers = []
            commitment_numbers = []
            
            # 从每个响应中提取数字
            for response in valid_responses:
                numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", response)
                if numbers:
                    response_numbers.append(numbers[-1])  # 使用最后一个数字作为答案
            
            # 从commitment中提取数字
            commit_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", commitment_text)
            if commit_numbers:
                commitment_numbers = commit_numbers[-1]  # 使用最后一个数字作为答案
            
            # 计算数字一致性奖励
            numerical_consistency = 0.0
            if response_numbers and commitment_numbers:
                # 检查所有响应的数字是否与commitment一致
                consistent_count = sum(1 for num in response_numbers if num == commitment_numbers)
                numerical_consistency = consistent_count / len(response_numbers)
            
            # 方法2: 基于响应与commitment的文本相似度
            text_similarity = 0.0
            if valid_responses and commitment_text:
                similarities = []
                for response in valid_responses:
                    # 检查响应长度，太短的响应（如错误消息）给予低分
                    if len(response.strip()) < 10:
                        similarities.append(0.0)
                    else:
                        sim = self._calculate_text_similarity(response, commitment_text)
                        similarities.append(sim)
                
                if similarities:
                    text_similarity = np.mean(similarities)
            
            # 方法3: 响应长度和质量评估
            quality_score = 0.0
            if valid_responses:
                quality_scores = []
                for response in valid_responses:
                    response_length = len(response.strip())
                    # 合理长度的响应得分更高
                    if 20 <= response_length <= 500:  # 合理长度范围
                        length_score = 1.0
                    elif 10 <= response_length <= 1000:  # 可接受范围
                        length_score = 0.7
                    else:  # 太短或太长
                        length_score = 0.3
                    
                    # 检查是否包含推理过程
                    has_reasoning = any(keyword in response.lower() 
                                      for keyword in ['first', 'then', 'therefore', 'because', 'so'])
                    reasoning_score = 0.3 if has_reasoning else 0.0
                    
                    quality_scores.append(length_score + reasoning_score)
                
                quality_score = np.mean(quality_scores)
            
            # 组合所有因素，数字一致性权重最高
            if numerical_consistency > 0:
                # 如果有数字一致性，主要基于此计算
                al_reward = 0.6 * numerical_consistency + 0.25 * text_similarity + 0.15 * quality_score
            else:
                # 如果没有数字一致性，主要基于文本相似度
                al_reward = 0.7 * text_similarity + 0.3 * quality_score
            
            return min(1.0, max(0.0, al_reward))
            
        except Exception as e:
            logger.warning(f"Error calculating AL reward: {e}")
            return 0.0

    def _calculate_collaborative_contribution_reward(self, final_answer: str, 
                                                   extra_info: Dict[str, Any], 
                                                   is_correct: bool) -> float:
        """
        计算协作贡献奖励 r^CC
        基于以下启发式规则：
        1. 如果最终答案正确且智能体响应多样化，给予高奖励
        2. 如果智能体响应与commitment一致，给予中等奖励
        3. 考虑响应的独特性和互补性
        4. 如果API失败或响应无效，返回0.0
        """
        try:
            agent_responses = extra_info.get('agent_responses', [])
            commitment_text = extra_info.get('commitment_text', '')
            
            # 检查是否有API错误
            error_indicators = ['Error: Could not generate response', 'API Error', 'HTTP error', 'Failed to generate']
            
            # 如果最终答案包含错误信息，返回0.0
            if any(error in str(final_answer) for error in error_indicators):
                return 0.0
            
            # 如果commitment包含错误信息，返回0.0
            if any(error in str(commitment_text) for error in error_indicators):
                return 0.0
            
            # 过滤掉包含错误的响应
            valid_responses = []
            for response in agent_responses:
                if not any(error in str(response) for error in error_indicators):
                    valid_responses.append(response)
            
            # 如果没有有效响应，返回0.0
            if not valid_responses:
                return 0.0
            
            # 基础奖励：正确性贡献（只有正确时才有基础奖励）
            base_reward = 0.3 if is_correct else 0.0
            
            # 多样性奖励：智能体响应的多样性（只考虑有效响应）
            diversity_reward = 0.0
            if len(valid_responses) > 1:
                unique_responses = len(set([resp.strip().lower() for resp in valid_responses]))
                diversity_ratio = unique_responses / len(valid_responses)
                diversity_reward = 0.3 * diversity_ratio
            
            # 一致性奖励：与commitment的一致性（只考虑有效响应）
            consistency_reward = 0.0
            if valid_responses and commitment_text:
                consistencies = []
                for response in valid_responses:
                    # 检查响应长度
                    if len(response.strip()) < 10:
                        consistencies.append(0.0)
                    else:
                        sim = self._calculate_text_similarity(response, commitment_text)
                        consistencies.append(sim)
                
                if consistencies:
                    avg_consistency = np.mean(consistencies)
                    consistency_reward = 0.2 * avg_consistency
            
            # 质量奖励：如果最终答案包含推理过程且不包含错误
            quality_reward = 0.0
            if final_answer and len(final_answer.strip()) > 10:
                # 简单启发式：长度合理且包含数学术语的答案质量更高
                answer_length = len(final_answer.split())
                has_reasoning = any(keyword in final_answer.lower() 
                                  for keyword in ['because', 'since', 'therefore', 'thus', 'so'])
                
                if 10 <= answer_length <= 200 and has_reasoning:
                    quality_reward = 0.2
                elif 5 <= answer_length <= 50:
                    quality_reward = 0.1
            
            total_cc_reward = base_reward + diversity_reward + consistency_reward + quality_reward
            return min(1.0, max(0.0, total_cc_reward))
            
        except Exception as e:
            logger.warning(f"Error calculating CC reward: {e}")
            return 0.0

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        计算两个文本之间的相似度
        使用TF-IDF向量和余弦相似度
        """
        try:
            if not text1.strip() or not text2.strip():
                return 0.0
            
            # 使用TF-IDF向量化
            vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)
            texts = [text1, text2]
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # 计算余弦相似度
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(max(0.0, min(1.0, similarity)))
            
        except Exception as e:
            logger.warning(f"Error calculating text similarity: {e}")
            return 0.0

    def get_env_info(self) -> Dict[str, Any]:
        # This info is used by the runner to setup the batch scheme.
        return {
            "episode_limit": self.episode_limit,
            "n_actions": 1,  # Placeholder - action space is text
            "obs_shape": (self.max_question_length,), # For scheme vshape
            "state_shape": (1,), # Placeholder for scheme vshape
            # Any other info needed by the runner or learner
        }

    def render(self, mode='human'):
        if mode == 'human':
            if self.current_sample:
                print("-" * 30)
                print(f"Current Question: {self.current_question}")
                print(f"Ground Truth Answer: {self.current_ground_truth_answer}")
                print("-" * 30)
            else:
                print("No current sample to render. Call reset() first.")

    def close(self):
        # Clean up resources if any (e.g., closing file handles if dataset was local)
        logger.info("Closing HuggingFaceDatasetEnv.")
        pass

# Example Usage (for testing purposes):
if __name__ == '__main__':
    env_args_gsm8k = {
        "hf_dataset_path": "gsm8k",
        "hf_dataset_config_name": "main",
        "dataset_split": "test",
        "question_field_name": "question",
        "answer_field_name": "answer",
        "max_question_length": 1024,
        "max_answer_length": 200,
        "dataset_streaming": False, # For testing, non-streaming is easier
        "use_random_sampling": False, # Disable random sampling for deterministic testing
        "use_dataset_episode": False # Disable dataset-level episode for deterministic testing
    }
    
    # env = HuggingFaceDatasetEnv(**env_args_gsm8k)
    # obs, info = env.reset()
    # print("Observation (Question):", obs)
    # print("Ground Truth (from info['sample']):", info['sample'][env.answer_field])
    
    # # Simulate a step
    # dummy_action = "The answer is \\boxed{10}." 
    # next_obs, reward, terminated, truncated, step_info = env.step(dummy_action)
    # print(f"LLM's Action: {dummy_action}")
    # print(f"Next Obs: {next_obs}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Step Info: {step_info}")
    # env.render()

    # env.reset() # Try another one
    # env.render()

    # Test MATH dataset
    env_args_math = {
        "hf_dataset_path": "competition_math",
        "dataset_split": "test", # Using test split which is smaller
        "question_field_name": "problem",
        "answer_field_name": "solution",
        "max_question_length": 2048,
        "max_answer_length": 2048,
        "dataset_streaming": False,
        "use_random_sampling": False, # Disable random sampling for deterministic testing
        "use_dataset_episode": False # Disable dataset-level episode for deterministic testing
    }
    math_env = HuggingFaceDatasetEnv(**env_args_math)
    obs, info = math_env.reset()
    math_env.render()
    # Simulate a step for MATH
    # For MATH, answers are more complex, often with LaTeX. Evaluation is harder.
    # Example: ground truth might be "\\boxed{-\\frac{1}{2}}"
    # dummy_math_action = "The final answer is \\boxed{-\\frac{1}{2}}"
    dummy_math_action = info['sample'][math_env.answer_field] # Give correct answer
    next_obs, reward, terminated, truncated, step_info = math_env.step(dummy_math_action)
    print(f"LLM's Action: {dummy_math_action}")
    print(f"Reward: {reward}") # Should be 1.0
    print(f"Step Info: {step_info}")
    math_env.render()

    # Test an incorrect answer for MATH
    obs, info = math_env.reset()
    math_env.render()
    dummy_math_action_wrong = "The final answer is \\boxed{42}"
    next_obs, reward, terminated, truncated, step_info = math_env.step(dummy_math_action_wrong)
    print(f"LLM's Action (Wrong): {dummy_math_action_wrong}")
    print(f"Reward: {reward}") # Should be 0.0
    print(f"Step Info: {step_info}")
    math_env.render() 