# fast_training.yaml - 用于快速训练和测试的配置

# 基础配置
runner: "episode_runner"
mac: "basic_mac"
learner: "q_learner"
t_max: 10                       # 运行10个steps用于测试完整推理过程
test_interval: 15               # 更频繁的测试
test_nepisode: 5                # 减少测试episode数量
batch_size_run: 1               # 保持批处理大小
n_agents: 3                     # 智能体数量
text_embed_dim: 1024 
belief_dim: 128 
state_shape: 1024  

# MAC
agent_output_type: "q_values"  
action_selector: "multinomial"  
obs_last_action: false  
obs_agent_id: true  
use_causal_mask: false  
max_seq_length: 1024  # same with max_question_length
n_actions: 2 

# LLM模型配置
llm_model_name: "gpt2"  # 仅用于tokenizer
together_api_key: "${TOGETHER_API_KEY}"  
coordinator_model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"  
executor_model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"  
commitment_embedding_model_name: "BAAI/bge-large-en-v1.5"

# Environment Settings
env: "huggingface_dataset_env"
env_args:
  hf_dataset_path: "gsm8k"
  hf_dataset_config_name: "main"
  dataset_split: "train"
  question_field_name: "question"
  answer_field_name: "answer"
  max_question_length: 512      # 减少最大问题长度
  max_answer_length: 100        # 减少最大答案长度
  dataset_streaming: false
  use_random_sampling: false    # 关闭随机采样，使用顺序遍历便于观察
  use_dataset_episode: true     # 启用数据集级别episode

# 训练配置
train:
  episodes_per_task: 10          # 每个任务的episode数（减少以便快速测试）
  buffer_size: 8                 # 减少缓冲区大小
  batch_size: 4                  # 减少批处理大小
  update_interval: 10            # 10步更新一次梯度（与用户要求一致）
  optimizer: "adam"
  learning_rate: 0.005
  coordinator_learning_rate: 0.0005
  gamma: 0.99

# 学习率配置 
lr: 0.005                       # 稍微提高学习率以便快速收敛
belief_net_lr: 0.005           # Belief网络学习率
encoder_lr: 0.005              # 编码器学习率
mixer_lr: 0.005                # Mixer学习率
weight_decay: 0.0
episode_length: 1

# BNE协调参数（用于快速训练）
bne_max_iterations: 3          # 减少BNE迭代次数以加快训练
bne_convergence_threshold: 0.02 # 放宽收敛条件
stage2_weight: 0.2             # 减少Stage 2的权重
bne_update_rate: 0.15          # 稍微提高BNE更新率

# 网络架构（简化）
arch:
  entity_dim: 128               # 减少实体维度
  attention_heads: 2            # 减少注意力头数
  transformer_blocks: 1         # 减少transformer层数
  key_dim: 64                   # 保持键维度
  mlp_hidden_size: 128          # 减少MLP隐藏层大小
  feedforward_size: 512         # 减少前馈网络大小
  dropout_rate: 0.1             # 保持dropout率
  layer_norm_epsilon: 0.00001

# Mixer相关配置
prompt_attention_heads: 2
commitment_embedding_dim: 1024

# 损失权重
loss:
  belief_weight: 0.1            # Belief网络损失权重
  encoder_weight: 0.1           # 编码器正则化权重
  mixing_weight: 0.1            # Mixer一致性权重

# 奖励配置
reward:
  max_value: 1.0
  initial_weights: [0.4, 0.4, 0.2]
  eta_alpha: 0.002              # 稍微提高alpha更新率
  dynamic_alpha_update: true
  r_expected_source: "q_value"
  al_weight: 0.3
  ts_weight: 0.5
  cc_weight: 0.2
  cc_reward_placeholder_value: 0.1

# 早停配置
early_stopping:
  commitment_threshold: 0.02     # 放宽承诺阈值
  loss_threshold: 0.001          # 放宽损失阈值
  reward_threshold: 0.5          # 降低奖励阈值
  patience: 3                    # 减少耐心值

# 系统配置
system:
  use_cuda: true
  device_num: 0
  seed: 42
  debug: true                    # 启用调试模式以显示详细推理过程

# 日志配置
logging:
  use_tensorboard: true
  log_interval: 1                # 每个step都记录，以便观察推理过程
  save_model: false              # 快速训练时不保存模型
  save_model_interval: 1000
  checkpoint_path: "./models/fast_training"
  log_path: "./logs/fast_training"
  experiment_name: "fast_training_10_steps_inference_test"

# 温度和采样控制
sampling:
  temperature_min: 0.2           # 提高最小温度
  temperature_max: 1.5           # 降低最大温度
  p_min: 0.2                     # 提高最小采样参数
  p_max: 0.8                     # 降低最大采样参数

# LLM配置
llm:
  together_api_key: "${TOGETHER_API_KEY}"
  coordinator_model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
  executor_model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
  max_tokens: 50                 # 严格限制到50个token，避免截断问题 